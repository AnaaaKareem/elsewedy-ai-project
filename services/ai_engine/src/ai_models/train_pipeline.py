import pandas as pd
import numpy as np
from model_factory import ModelFactory

"""
Offline Training Pipeline.

This script iterates through the historical CSV dataset (exported by data_exporter.py),
selects appropriate proxy materials for each category, and triggers the full training
process (Batch Training) for the relevant AI models.
"""

def train_all_models(csv_path):
    print(f"üìÇ Loading data from {csv_path}...")
    try:
        df = pd.read_csv(csv_path)
    except Exception as e:
        print(f"‚ùå Error reading CSV: {e}")
        return

    # Ensure date sorting
    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'])
        df = df.sort_values('Date')

    # Get unique categories in the CSV
    categories = df['Category'].unique()

    factory = ModelFactory()

    for category in categories:
        print(f"\n--- Processing Category: {category} ---")
        subset = df[df['Category'] == category]
        
        if subset.empty:
            continue

        model = factory.get_model(category)
        if model is None: continue

        # --- PILLAR 2: SHIELDING (LSTM) ---
        if category == 'Shielding':
            # Train on 'Copper' as Proxy
            rep_subset = subset[subset['Material'] == 'Copper']
            if rep_subset.empty: rep_subset = subset

            prices = rep_subset['Price'].values
            print(f"   Training LSTM on {len(prices)} rows (Proxy: {rep_subset['Material'].iloc[0]})...")
            model.train_from_history(prices, epochs=50, batch_size=64)
            model.save_weights("shielding_lstm")

        # --- PILLAR 1: POLYMER (XGBoost) ---
        elif category == 'Polymer':
            # Train on 'PVC' as Proxy
            rep_subset = subset[subset['Material'] == 'PVC']
            if rep_subset.empty: rep_subset = subset

            # Check columns
            # Generate Lag Features for Autoregression
            # Since 'Driver_Value' and 'Construction_Index' are removed, we rely on Price lags.
            target_col = 'Price'
            
            # Create Lag Features (e.g. Lag1, Lag2)
            # This allows XGBoost to learn temporal patterns even without LSTM's state.
            proxy_df = rep_subset.copy()
            proxy_df['Lag1'] = proxy_df['Price'].shift(1)
            proxy_df['Lag2'] = proxy_df['Price'].shift(2)
            proxy_df['Lag7'] = proxy_df['Price'].shift(7)
            
            # Drop NaN rows created by shifting
            proxy_df = proxy_df.dropna()
            
            if proxy_df.empty:
                print("‚ùå Not enough data for lag generation.")
                continue

            feats = ['Lag1', 'Lag2', 'Lag7']
            
            X = proxy_df[feats].values
            y = proxy_df[target_col].values
            y = rep_subset[target_col].values

            print(f"   Training XGBoost on {len(X)} rows (Proxy: {rep_subset['Material'].iloc[0]})...")
            model.train(X, y)
            model.save_weights("polymer_xgb")

        # --- PILLAR 3: SCREENING (Croston) ---
        elif category == 'Screening':
            # Train on 'Mica Tape' as Proxy (Intermittent)
            rep_subset = subset[subset['Material'] == 'Mica Tape']
            if rep_subset.empty: rep_subset = subset

            col = 'Demand' if 'Demand' in rep_subset.columns else 'Demand_Qty'
            if col not in rep_subset.columns:
                 print("‚ùå Demand column missing.")
                 continue

            history = rep_subset[col].values
            print(f"   Training Croston on {len(history)} rows (Proxy: {rep_subset['Material'].iloc[0]})...")
            model.train(history)
            model.save_weights("screening_lumpy")

    print("\n‚úÖ All training tasks completed.")

if __name__ == "__main__":
    # Uses the CSV generated by data_exporter.py
    train_all_models("sentinel_training_data.csv")